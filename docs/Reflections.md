# Reflection
>每天有一些本质上的思考，让每一天显得没那么没用。

## 2025-12-01
以前一直认为KV cache的驱逐是以注意力分数为唯一标准的，今天读了PyramidKV发现层的深度也很重要，层越浅，注意力分数的差别并没有那么明显。

## 2025-12-02
现在想来，vllm的核心功能还是在model.generate上，这是它的本质。至于说为什么那么复杂，是因为需要处理好各个sequence的并发问题，地址空间管理，还有调度问题。多种问题的解决，就需要一个系统性的工程了。
同样的，ai infra那么多复杂的工程，本质也在一句话上，把各种bound变成compute bound，把算力拉满。后面理解那些复杂的系统多从这个角度想想，或许会有不错的帮助。（ref： 知乎上的博主SiriusNEO）

